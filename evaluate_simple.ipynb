{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "from dotenv import load_dotenv\n",
    "import openai\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import json\n",
    "from ipywidgets import IntProgress\n",
    "from IPython.display import display\n",
    "from scipy import stats\n",
    "from scikit_posthocs import posthoc_dunn\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to the results directory\n",
    "results_dir = \"results\"\n",
    "\n",
    "# Function to extract model name from filename\n",
    "def extract_model_name(filename):\n",
    "    match = re.search(r\"experimental_design_results_(.*)\\.csv\", filename)\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "    else:\n",
    "        return \"Unknown Model\"\n",
    "\n",
    "# Function to read CSV files from a directory, add 'model' column, and select specific columns\n",
    "def read_and_label_csvs(directory):\n",
    "    dfs = []\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith(\".csv\"):\n",
    "            filepath = os.path.join(directory, filename)\n",
    "            try:\n",
    "                df = pd.read_csv(filepath, encoding='utf-8')\n",
    "            \n",
    "                model_name = extract_model_name(filename)\n",
    "                df['model'] = model_name  # Add model name as a column\n",
    "                \n",
    "                # Select specific columns\n",
    "                df = df[['model', 'baseline', 'results']]\n",
    "                \n",
    "                dfs.append(df)\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading {filename}: {e}\")\n",
    "    return dfs\n",
    "\n",
    "# Read and label CSV files from the results directory\n",
    "df = read_and_label_csvs(results_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the path to your .env file\n",
    "dotenv_path = \"/mnt/4d4f90e5-f220-481e-8701-f0a546491c35/arquivos/projetos/.env\"\n",
    "\n",
    "# Load the .env file\n",
    "load_dotenv(dotenv_path=dotenv_path)\n",
    "\n",
    "# Access and store the environment variable\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "model = 'gpt-4o-mini-2024-07-18'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fe284810d8b46429ac8ff71cb5eeada",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntProgress(value=0, bar_style='info', description='Evaluating', max=3)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting list of DataFrames into a single DataFrame.\n",
      "min. pause...\n",
      "Completed. Results saved in final_evaluation_simple.csv\n"
     ]
    }
   ],
   "source": [
    "# Initialize progress bar\n",
    "progress_bar = IntProgress(min=0, max=len(df), description='Evaluating', bar_style='info')\n",
    "display(progress_bar)\n",
    "\n",
    "# Ensure df is a DataFrame\n",
    "if isinstance(df, list):\n",
    "    print(\"Converting list of DataFrames into a single DataFrame.\")\n",
    "    df = pd.concat(df, ignore_index=True)\n",
    "\n",
    "# Iterate over each row of the DataFrame\n",
    "for index, row in df.iterrows():\n",
    "    if index % 150 == 0 and index != 0:\n",
    "        print(\"min. pause...\")\n",
    "        time.sleep(60)\n",
    "    \n",
    "    baseline = row['baseline']\n",
    "    result = row['results']\n",
    "    temperature = 0.0\n",
    "    \n",
    "    comparison_promt = f\"\"\"\n",
    "        Atribua uma nota de 0 a 10 para resposta 'Modelo' (a ser avaliada) em comparação com a resposta 'Baseline' (resposta correta). \n",
    "        Respostas do modelo que indicam falta de acesso a informações específicas e recomendam consultar fontes externas (ex: \"Desculpe, mas não tenho acesso a informações específicas como CEPs...\") devem ser penalizada. \n",
    "        Não penalize a avaliação em caso de repetições no texto. Retorne a avaliação no formato JSON, com a chave \"evaluation\" e \"justification\".\n",
    "\n",
    "        Resposta Base (Baseline): {baseline}\n",
    "        Resposta do Modelo: {result}\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        response = openai.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[{\"role\": \"user\", \"content\": comparison_promt}],\n",
    "            temperature=temperature\n",
    "        )\n",
    "        generated_text = response.choices[0].message.content\n",
    "        \n",
    "        # Process the JSON response\n",
    "        metadata_str = generated_text.strip()\n",
    "        if metadata_str.startswith(\"```json\") and metadata_str.endswith(\"```\"):\n",
    "            metadata_str = metadata_str.strip(\"```json\").strip()\n",
    "        \n",
    "        try:\n",
    "            data = json.loads(metadata_str)\n",
    "            # Update DataFrame with the extracted values\n",
    "            df.loc[index, 'evaluation'] = data.get('evaluation')\n",
    "            df.loc[index, 'justification'] = data.get('justification')\n",
    "        \n",
    "        except json.JSONDecodeError as e:\n",
    "            # Print error message if JSON decoding fails\n",
    "            print(f\"Error decoding JSON: {e}\")\n",
    "            print(f\"Problematic JSON string: {metadata_str}\")\n",
    "            df.loc[index, 'evaluation'] = None\n",
    "            df.loc[index, 'justification'] = f\"JSON Error: {e}\"\n",
    "\n",
    "    except Exception as e:\n",
    "        # Print error message if there's an issue processing the line\n",
    "        print(f\"Error processing line {index}: {e}\")\n",
    "        df.loc[index, 'evaluation'] = None\n",
    "        df.loc[index, 'justification'] = f\"Error: {e}\"\n",
    "    \n",
    "    progress_bar.value += 1\n",
    "        \n",
    "# Save the updated DataFrame\n",
    "output_filename = \"final_evaluation_simple.csv\"\n",
    "df.to_csv(output_filename, index=False)\n",
    "print(f\"Completed. Results saved in {output_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      model  evaluation\n",
      "0    gpt-4o-mini-2024-07-18         2.0\n",
      "1    gpt-4o-mini-2024-07-18         2.0\n",
      "2    gpt-4o-mini-2024-07-18         2.0\n",
      "3    gpt-4o-mini-2024-07-18         2.0\n",
      "4    gpt-4o-mini-2024-07-18         2.0\n",
      "..                      ...         ...\n",
      "295          TeenyTinyLlama         0.0\n",
      "296          TeenyTinyLlama         1.0\n",
      "297          TeenyTinyLlama         0.0\n",
      "298          TeenyTinyLlama         0.0\n",
      "299          TeenyTinyLlama         1.0\n",
      "\n",
      "[300 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"final_evaluation_simple.csv\")\n",
    "df_final = df[['model', 'evaluation']].copy()\n",
    "\n",
    "print(df_final)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# H0: The medians of all groups are equal.\n",
    "# H1: At least one group median is different from the others.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap_mean_ci(data, n_iterations=1000, confidence_level=0.95):\n",
    "    \"\"\"Calculates the bootstrap confidence interval for the mean.\"\"\"\n",
    "    means = []\n",
    "    for _ in range(n_iterations):\n",
    "        sample = np.random.choice(data, size=len(data), replace=True)\n",
    "        means.append(np.mean(sample))\n",
    "    \n",
    "    alpha = (1 - confidence_level) / 2\n",
    "    lower_percentile = alpha * 100\n",
    "    upper_percentile = (1 - alpha) * 100\n",
    "    \n",
    "    lower_bound = np.percentile(means, lower_percentile)\n",
    "    upper_bound = np.percentile(means, upper_percentile)\n",
    "    \n",
    "    return lower_bound, upper_bound\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary evaluation:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>std_err</th>\n",
       "      <th>min</th>\n",
       "      <th>max</th>\n",
       "      <th>median</th>\n",
       "      <th>ci_lower</th>\n",
       "      <th>ci_upper</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>TeenyTinyLlama</th>\n",
       "      <td>0.39</td>\n",
       "      <td>0.063397</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.51025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TeenyTinyLlama-160m-CEP-ft</th>\n",
       "      <td>1.90</td>\n",
       "      <td>0.067420</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.77</td>\n",
       "      <td>2.02000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt-4o-mini-2024-07-18</th>\n",
       "      <td>1.92</td>\n",
       "      <td>0.039389</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.84</td>\n",
       "      <td>1.98000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            mean   std_err  min  max  median  ci_lower  \\\n",
       "model                                                                    \n",
       "TeenyTinyLlama              0.39  0.063397  0.0  2.0     0.0      0.27   \n",
       "TeenyTinyLlama-160m-CEP-ft  1.90  0.067420  0.0  3.0     2.0      1.77   \n",
       "gpt-4o-mini-2024-07-18      1.92  0.039389  0.0  2.0     2.0      1.84   \n",
       "\n",
       "                            ci_upper  \n",
       "model                                 \n",
       "TeenyTinyLlama               0.51025  \n",
       "TeenyTinyLlama-160m-CEP-ft   2.02000  \n",
       "gpt-4o-mini-2024-07-18       1.98000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Kruskal-Wallis Test: KruskalResult(statistic=np.float64(187.76073097873862), pvalue=np.float64(1.6915128793802754e-41)) (*)\n",
      "\n",
      "Dunn's Test (Bonferroni correction):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TeenyTinyLlama</th>\n",
       "      <th>TeenyTinyLlama-160m-CEP-ft</th>\n",
       "      <th>gpt-4o-mini-2024-07-18</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>TeenyTinyLlama</th>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>8.861055e-32</td>\n",
       "      <td>3.161980e-32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TeenyTinyLlama-160m-CEP-ft</th>\n",
       "      <td>8.861055e-32</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt-4o-mini-2024-07-18</th>\n",
       "      <td>3.161980e-32</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            TeenyTinyLlama  TeenyTinyLlama-160m-CEP-ft  \\\n",
       "TeenyTinyLlama                1.000000e+00                8.861055e-32   \n",
       "TeenyTinyLlama-160m-CEP-ft    8.861055e-32                1.000000e+00   \n",
       "gpt-4o-mini-2024-07-18        3.161980e-32                1.000000e+00   \n",
       "\n",
       "                            gpt-4o-mini-2024-07-18  \n",
       "TeenyTinyLlama                        3.161980e-32  \n",
       "TeenyTinyLlama-160m-CEP-ft            1.000000e+00  \n",
       "gpt-4o-mini-2024-07-18                1.000000e+00  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "evaluation = df_final.groupby('model')['evaluation'].agg(['mean', 'sem', 'min', 'max', 'median'])\n",
    "\n",
    "# Calculate bootstrap confidence intervals\n",
    "evaluation['ci_lower'] = df_final.groupby('model')['evaluation'].apply(lambda x: bootstrap_mean_ci(x.dropna())[0])\n",
    "evaluation['ci_upper'] = df_final.groupby('model')['evaluation'].apply(lambda x: bootstrap_mean_ci(x.dropna())[1])\n",
    "\n",
    "\n",
    "# Rename columns for better clarity\n",
    "summary_evaluation = evaluation.rename(columns={\n",
    "    'mean': 'mean',\n",
    "    'sem': 'std_err',\n",
    "    'min': 'min',\n",
    "    'max': 'max',\n",
    "    'median': 'median'\n",
    "})\n",
    "\n",
    "# Print the summary DataFrame\n",
    "print(\"Summary evaluation:\")\n",
    "display(summary_evaluation)\n",
    "\n",
    "# Kruskal-Wallis Test\n",
    "kruskal_result = stats.kruskal(*[group['evaluation'].dropna().values for name, group in df_final.groupby('model')])\n",
    "significance = \" (*)\" if kruskal_result.pvalue < 0.05 else \"\"\n",
    "print(f\"\\nKruskal-Wallis Test: {kruskal_result}{significance}\")\n",
    "\n",
    "# Dunn's Test (post-hoc)\n",
    "if kruskal_result.pvalue < 0.05:\n",
    "    dunn_result = posthoc_dunn(df_final, val_col='evaluation', group_col='model', p_adjust='bonferroni')\n",
    "    print(\"\\nDunn's Test (Bonferroni correction):\")\n",
    "    display(dunn_result)\n",
    "else:\n",
    "    print(\"Kruskal-Wallis test is not significant, skipping Dunn's test.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
